ğŸš€ Production-Ready ML Inference System with A/B Testing

This repository implements a production-grade machine learning inference system designed for scalability, reliability, and controlled experimentation (A/B testing).

The system serves deep learning image classification models for Tomato Leaf Disease Detection using a Dockerized microservices architecture.

ğŸ§  Key Features

âœ… Versioned ML models (A/B testing ready)

âœ… Asynchronous inference with Redis

âœ… FastAPI-based REST API

âœ… Background worker for model inference

âœ… NGINX reverse proxy

âœ… Docker & Docker Compose orchestration

âœ… Frontend UI for image upload

âœ… Separation of training & inference

âœ… Production-safe model loading

ğŸ—ï¸ System Architecture
Client (Browser)
        â”‚
        â–¼
     NGINX
        â”‚
        â–¼
   FastAPI API
        â”‚
        â–¼
     Redis Queue
        â”‚
        â–¼
 Background Worker
        â”‚
        â–¼
   Versioned ML Model

ğŸ“ Project Structure
ml-inference-system/
â”‚
â”œâ”€â”€ app/                     # FastAPI application
â”‚   â”œâ”€â”€ main.py              # API endpoints
â”‚   â”œâ”€â”€ model.py             # Versioned model loader
â”‚   â”œâ”€â”€ preprocess.py        # Image preprocessing
â”‚   â”œâ”€â”€ tasks.py             # Task enqueue logic
â”‚   â”œâ”€â”€ redis_conn.py        # Redis connection
â”‚
â”œâ”€â”€ worker/                  # Background inference worker
â”‚   â””â”€â”€ worker.py
â”‚
â”œâ”€â”€ models/                  # Versioned ML models
â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â””â”€â”€ Tomato_model_v1.h5
â”‚   â””â”€â”€ v2/
â”‚       â””â”€â”€ Tomato_model_v2.h5
â”‚
â”œâ”€â”€ templates/               # Frontend UI
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css
â”‚
â”œâ”€â”€ nginx/                   # Reverse proxy
â”‚   â””â”€â”€ nginx.conf
â”‚
â”œâ”€â”€ Dockerfile.api           # FastAPI container
â”œâ”€â”€ Dockerfile.worker        # Worker container
â”œâ”€â”€ docker-compose.yml       # Multi-service orchestration
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ”€ Model Versioning & A/B Testing

Models are stored in versioned directories (v1, v2)

Same API endpoint can route traffic to different models

Enables:

Performance comparison

Accuracy vs latency trade-offs

Safe production experiments

Example loader:

load_model("v1")
load_model("v2")


ğŸ‘‰ Model version is never exposed to frontend users

âš™ï¸ How Inference Works

User uploads an image via UI or API

FastAPI enqueues request to Redis

Worker picks up the task asynchronously

Model performs inference

Prediction is returned to the user

This design:

Prevents API blocking

Supports concurrent requests

Scales horizontally

ğŸŒ NGINX Reverse Proxy

NGINX:

Routes traffic to FastAPI

Acts as a single entry point

Enables future SSL / load balancing

location / {
    proxy_pass http://api:8000;
}

ğŸ³ Dockerized Deployment
Why Docker?

Environment consistency

Easy scaling

Production-ready isolation

Services
Service	Purpose
api	Handles HTTP requests
worker	Performs ML inference
redis	Message queue
nginx	Reverse proxy
â–¶ï¸ How to Run (Production Mode)
Prerequisites

Docker

Docker Compose

Start the system
docker-compose up --build

Access the app

Frontend: http://localhost

API: http://localhost/docs

âŒ No virtual environment required
âŒ No manual dependency installation

ğŸ“¦ Requirements
Python 3.10
TensorFlow
FastAPI
Redis
Uvicorn
Pillow
NumPy


All dependencies are handled inside Docker containers.

ğŸ” Production-Safe Design Decisions

compile=False when loading models

Background inference workers

Stateless API layer

Version-controlled models

No training code inside inference pipeline

ğŸ“ˆ Performance & Scalability

Handles concurrent requests

Worker pool can be horizontally scaled

Redis ensures fault-tolerant task handling

Easily extendable to:

Kubernetes

Cloud deployment (AWS / GCP / Azure)

ğŸ§ª Training Pipeline

Training notebooks are intentionally separated from production inference code.

ğŸ“‚ See /training directory for:

Dataset link

Model architectures

Training details

Accuracy metrics

ğŸ‘¨â€ğŸ’» Author

Arindam Das
Machine Learning / AI Engineer

This project demonstrates:

ML system design

Production deployment

A/B testing mindset

End-to-end ML ownership

â­ Why This Project Matters

This is not just a CNN model.

It is a real-world ML system showing:

How models are served in production

How experiments are run safely

How ML meets DevOps