# ğŸš€ Production-Ready ML Inference System with A/B Testing

This repository implements a production-grade machine learning inference system designed for scalability, reliability, and controlled experimentation (A/B testing). 

The system serves deep learning image classification models for **Tomato Leaf Disease Detection** using a Dockerized microservices architecture.

---

## ğŸ§  Key Features

* âœ… **Versioned ML models**: Ready for A/B testing and seamless rollouts.
* âœ… **Asynchronous inference**: Powered by Redis to handle heavy workloads without blocking.
* âœ… **FastAPI-based REST API**: High-performance interface for model interaction.
* âœ… **Background Worker**: Dedicated processing for model inference logic.
* âœ… **NGINX Reverse Proxy**: Single entry point for security and load balancing.
* âœ… **Docker Orchestration**: Fully containerized using Docker Compose.
* âœ… **Frontend UI**: Simple interface for user-friendly image uploads.
* âœ… **Production-Safe Design**: Separation of training and inference logic.

---

## ğŸ—ï¸ System Architecture



```text
Client (Browser) 
      â”‚ 
      â–¼ 
    NGINX (Reverse Proxy)
      â”‚ 
      â–¼ 
   FastAPI (API Layer) 
      â”‚ 
      â–¼ 
   Redis (Task Queue) 
      â”‚ 
      â–¼ 
 Background Worker (Inference Engine)
      â”‚ 
      â–¼ 
 Versioned ML Models (H5/SavedModel
 
ğŸ“ Project StructurePlaintextml-inference-system/
â”‚
â”œâ”€â”€ app/                    # FastAPI application
â”‚   â”œâ”€â”€ main.py             # API endpoints
â”‚   â”œâ”€â”€ model.py            # Versioned model loader
â”‚   â”œâ”€â”€ preprocess.py       # Image preprocessing
â”‚   â”œâ”€â”€ tasks.py            # Task enqueue logic
â”‚   â””â”€â”€ redis_conn.py       # Redis connection
â”‚
â”œâ”€â”€ worker/                 # Background inference worker
â”‚   â””â”€â”€ worker.py
â”‚
â”œâ”€â”€ models/                 # Versioned ML models
â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â””â”€â”€ Tomato_model_v1.h5
â”‚   â””â”€â”€ v2/
â”‚       â””â”€â”€ Tomato_model_v2.h5
â”‚
â”œâ”€â”€ templates/              # Frontend UI
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ static/                 # CSS/JS Assets
â”‚   â””â”€â”€ style.css
â”‚
â”œâ”€â”€ nginx/                  # Reverse proxy configuration
â”‚   â””â”€â”€ nginx.conf
â”‚
â”œâ”€â”€ Dockerfile.api          # FastAPI container definition
â”œâ”€â”€ Dockerfile.worker       # Worker container definition
â”œâ”€â”€ docker-compose.yml      # Multi-service orchestration
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project documentation

ğŸ”€ Model Versioning & A/B TestingModels are stored in versioned directories (v1, v2). The system allows routing traffic to different model versions to enable:
       Performance comparison in real-world scenarios.
       Accuracy vs. Latency trade-off analysis.
       Safe experiments without exposing internal versioning to the end-user.

Example Internal Loader:
model_v1 = load_model("models/v1/Tomato_model_v1.h5")
model_v2 = load_model("models/v2/Tomato_model_v2.h5")


âš™ï¸ How Inference WorksUpload:
       User uploads an image via the UI or API.
       Enqueue: FastAPI receives the file and enqueues a task into Redis.
       Process: The Worker picks up the task, performs preprocessing, and runs inference.
       Result: The prediction result is returned asynchronously or stored for retrieval.
       Benefits: Prevents API timeouts, supports high concurrency, and allows horizontal scaling of workers.


ğŸŒ NGINX Reverse ProxyNGINX acts as the gateway, routing traffic to the internal API and serving as a foundation for future SSL termination and load balancing.

Nginx
location / {
    proxy_pass http://api:8000;
}

ğŸ³ Dockerized Deployment
Service,Purpose
api,Handles HTTP requests and task delegation.
worker,Performs heavy ML inference.
redis,Acts as the message broker (Queue).
nginx,Reverse proxy and static file server.


â–¶ï¸ How to Run
Prerequisites: Docker & Docker Compose installed.
   1. Clone the repo
   2. Start the system:
            docker-compose up --build

   3. Access the application:
           Frontend: http://localhost
           API Docs: http://localhost/docs


ğŸ” Production-Safe Design Decisions
        compile=False: Used during model loading to save memory and avoid training overhead.
        Decoupled Architecture: Training code is kept strictly separate from the inference pipeline.
        Statelessness: The API layer can be scaled infinitely as it holds no state.


ğŸ‘¨â€ğŸ’» Author
Arindam Das - Machine Learning / AI Engineer
This project demonstrates expertise in ML System Design, Production Deployment, and an End-to-End Ownership mindset.


â­ If you find this project useful, please consider giving it a star!

